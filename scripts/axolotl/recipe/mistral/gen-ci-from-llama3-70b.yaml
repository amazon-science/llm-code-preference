# SPDX-FileCopyrightText: (c) 2024 Amazon.com, Inc. or its affiliates
#
# SPDX-License-Identifier: CC-BY-NC-4.0

## Base model
base_model: mistralai/Mistral-7B-Instruct-v0.3
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer

############################################
### Configs that you may want to change  ###
datasets:
  - path: ./storage/datasets/cci/editpackft-chatcommit-v4-raw-meta-llama--Meta-Llama-3-70B-Instruct.axolotl.cot.both_order
    conversation: mistral
    type: sharegpt.load_ultrachat
    split: "train"
    train_on_split: "train"

learning_rate: 5.0e-6
gradient_accumulation_steps: 1
micro_batch_size: 4
num_epochs: 2

seed: 666
output_dir: ./storage/repro-models-1/ci-gen-mistral-7b-it_lr5e-6-l3-70b
wandb_name: "ci-gen-mistral-7b-it_lr5e-6-l3-70b"
sequence_len: 2048
############################################

## Input sequences
train_on_inputs: false
sample_packing: true
pad_to_sequence_len: true
group_by_length: false
dataset_processes: 48

## Vanilla
save_safetensors: true
bf16: auto
fp16: false
tf32: true
logging_steps: 4
flash_attention: true

## Optimizer
optimizer: adamw_torch_fused
lr_scheduler: cosine
gradient_checkpointing: True
weight_decay: 0.0
max_grad_norm: 1.0
warmup_steps: 40

## Others
save_strategy: "no"
wandb_project: llm4code
special_tokens:
  pad_token: "</s>"
